{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering - By Foivos Gaitantzis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section of the program is dedicated to downloading historical stock data (from Yahoo) and scraping news headlines from three different sources (Business Standard, The Financial Times & The New York Times) for a given datarange. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Query Parameters & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Query Keywords to the List\n",
    "query_list = ['aapl', 'apple+inc']\n",
    "\n",
    "#Add Filter Keywords for Headlines\n",
    "filter_keywords = ['aapl', 'iphone', 'ipad', 'apple', 'app', 'stock', 'aal', 'mac', 'steve']\n",
    "\n",
    "#Set the Start Date\n",
    "start_month = 1\n",
    "start_day = 1\n",
    "start_year = 2016\n",
    "\n",
    "#Set the End Date\n",
    "end_month = 12\n",
    "end_day = 31\n",
    "end_year = 2018\n",
    "\n",
    "#---- New York Times API Parameters ----#\n",
    "\n",
    "#NYTimes API Key\n",
    "API_Key = \"HIDDEN\"\n",
    "\n",
    "#NYTimes Date Interval Count (k)\n",
    "Interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the date range between two dates\n",
    "def daterange(start, end):\n",
    "    for i in range((end - start).days + 1):\n",
    "        yield (start + datetime.timedelta(i))\n",
    "        \n",
    "#Get the date range between two dates for given intervals        \n",
    "def daterangeintv(start, end, intv):\n",
    "    for i in range(intv):\n",
    "        yield (start + (end - start) / intv * i)\n",
    "    yield end\n",
    "        \n",
    "#Define the two dates in datetime format\n",
    "start_date = datetime.date(year = start_year, month = start_month, day = start_day)\n",
    "end_date = datetime.date(year = end_year, month = end_month, day = end_day)\n",
    "\n",
    "#Define the Requests Headers\n",
    "Headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Historical Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>105.349998</td>\n",
       "      <td>67649400</td>\n",
       "      <td>98.213585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>102.709999</td>\n",
       "      <td>55791000</td>\n",
       "      <td>95.752419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>100.699997</td>\n",
       "      <td>68457400</td>\n",
       "      <td>93.878586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>96.449997</td>\n",
       "      <td>81094400</td>\n",
       "      <td>89.916473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>96.959999</td>\n",
       "      <td>70798000</td>\n",
       "      <td>90.391907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Close    Volume  Adj Close\n",
       "Date                                       \n",
       "2016-01-04  105.349998  67649400  98.213585\n",
       "2016-01-05  102.709999  55791000  95.752419\n",
       "2016-01-06  100.699997  68457400  93.878586\n",
       "2016-01-07   96.449997  81094400  89.916473\n",
       "2016-01-08   96.959999  70798000  90.391907"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data for the stock AAPL\n",
    "Stock_Data = yf.download('AAPL', start_date, end_date)\n",
    "Stock_Data = Stock_Data[['Close', 'Volume', 'Adj Close']]\n",
    "\n",
    "#Extract the dataframe to a CSV file\n",
    "Stock_Data.to_csv('files/'+query_list[0]+'_Stock_Data.csv')\n",
    "\n",
    "#Display the first 5 rows of the Historical Stock Data\n",
    "Stock_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Standard News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prepare the Pandas Dataframe that will host the Business Standard News\n",
    "PD_Headers = pd.DataFrame({'Date': [], 'Headline': []})\n",
    "\n",
    "#Loop through each Date (day-to-day)\n",
    "for Date in daterange(start_date, end_date):\n",
    "    #Loop through each query on the query list\n",
    "    for query in query_list:\n",
    "    \n",
    "        #Define the Business Standard Scraping URL\n",
    "        URL = 'https://www.business-standard.com/advance-search?type=news&c-q=q&q='+query+'&c-range=range&range=bwn_dates&from_date='+Date.strftime(\"%d-%m-20%y\")+'&to_date='+Date.strftime(\"%d-%m-20%y\")\n",
    "\n",
    "        #Request the URL and Retrieve the Headlines\n",
    "        raw_html = requests.get(URL, headers=Headers)\n",
    "        soup = BeautifulSoup(raw_html.text, 'lxml')\n",
    "        raw = soup.find(\"ul\", class_=\"listing\")\n",
    "        headline_list = raw.find_all(\"a\", href=re.compile(\"/article/\"))\n",
    "\n",
    "        #Define the Date & Headline Lists\n",
    "        dates = []\n",
    "        headlines = []\n",
    "\n",
    "        #Loop through each element in the headline list (for that specific query)\n",
    "        for elem in headline_list:\n",
    "            headlines.append(elem.text)\n",
    "                \n",
    "        #Loop through each element in the headline list (for that specific query)\n",
    "        for elem in headline_list:\n",
    "            dates.append(Date)\n",
    "\n",
    "        #Create a new Dataframe that stores the data of this specific query\n",
    "        News = pd.DataFrame({'Date': dates, 'Headline': headlines})\n",
    "        \n",
    "        #Concatanate this dataframe with the one containing ALL Business Standard Headlines\n",
    "        PD_Headers = pd.concat([PD_Headers, News])\n",
    "    \n",
    "#Remove any possible duplicates\n",
    "PD_Headers.drop_duplicates(['Headline'], keep='last')\n",
    "\n",
    "#Filter the Keywords\n",
    "PD_Headers = PD_Headers[PD_Headers['Headline'].str.contains('|'.join(filter_keywords), case=False)]\n",
    "\n",
    "#Extract the dataframe to a CSV file\n",
    "PD_Headers.to_csv('files/'+query_list[0]+'_News_BS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New York Times News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create a list of dates using Intervals between start date and end date\n",
    "Datelist = list(daterangeintv(start_date, end_date, Interval))\n",
    "\n",
    "#Prepare the Pandas Dataframe that will host the New York Times News\n",
    "PD_Headers = pd.DataFrame({'Date': [], 'Headline': []})\n",
    "\n",
    "#Utility Function to access API\n",
    "def access_api(query, page, start_date, end_date):\n",
    "    \n",
    "    #CPU sleep for 1s\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #Define the NYTimes Scraping URL\n",
    "    URL = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q='+query+'&sort=relevance&fq='+query+'&page='+str(page)+'&api-key='+API_Key+'&begin_date='+start_date.strftime(\"%Y%m%d\")+'&end_date='+end_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    raw_html = None\n",
    "    content = None\n",
    "    i = 0\n",
    "    \n",
    "    #Attempt to Request page 3 times without getting any errors.\n",
    "    while i<3 and (content is None or raw_html.status_code!=200):\n",
    "        try:\n",
    "            raw_html = requests.get(URL)\n",
    "            data = json.loads(raw_html.content.decode(\"utf-8\"))\n",
    "            content = data[\"response\"]\n",
    "        except ValueError:\n",
    "            raise ValueError\n",
    "        except KeyError:\n",
    "            continue\n",
    "        i += 1\n",
    "    \n",
    "    #Return the page content\n",
    "    return content\n",
    "\n",
    "#Loop through each Date Interval\n",
    "for i in range(1, Interval):\n",
    "    #Loop through each query on the query list\n",
    "    for query in query_list:\n",
    "        \n",
    "        #Define the Date & Headline Lists\n",
    "        dates = []\n",
    "        headlines = []\n",
    "        page = 0\n",
    "        \n",
    "        #Fetch content from API and store in the lists\n",
    "        Data = access_api(query, page, Datelist[i-1], Datelist[i])\n",
    "        while Data[\"meta\"][\"hits\"] > 1000:\n",
    "            Data = access_api(query, page, Datelist[i-1], Datelist[i])\n",
    "        while page * 10 < Data[\"meta\"][\"hits\"] and (page + 1) < 100:\n",
    "            Data = access_api(query, page, Datelist[i-1], Datelist[i])\n",
    "            for doc in Data[\"docs\"]:\n",
    "                headlines.append(doc['headline']['main'])\n",
    "                dates.append(doc['pub_date'][0:10])\n",
    "            page += 1\n",
    "        \n",
    "        #Create a new Dataframe that stores the data of this specific query\n",
    "        News = pd.DataFrame({'Date': dates, 'Headline': headlines})\n",
    "            \n",
    "        #Concatanate this dataframe with the one containing ALL NYTimes News\n",
    "        PD_Headers = pd.concat([PD_Headers, News])\n",
    "        \n",
    "#Remove any possible duplicates\n",
    "PD_Headers.drop_duplicates(['Headline'], keep='last')\n",
    "\n",
    "#Filter the Keywords\n",
    "PD_Headers = PD_Headers[PD_Headers['Headline'].str.contains('|'.join(filter_keywords), case=False)]\n",
    "\n",
    "#Extract the dataframe to a CSV file\n",
    "PD_Headers.to_csv('files/'+query_list[0]+'_News_NYTimes.csv')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial Times News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the Pandas Dataframe that will host the Financial Times News\n",
    "PD_Headers = pd.DataFrame({'Date': [], 'Headline': []})\n",
    "\n",
    "#Loop through each Date (day-to-day)\n",
    "for Date in daterange(start_date, end_date):\n",
    "    #Loop through each query on the query list\n",
    "    for query in query_list:\n",
    "        #Loop through the first 3 pages\n",
    "        for page in range(1, 3):\n",
    "    \n",
    "            #Define the Google News Scraping URL\n",
    "            URL = 'https://www.ft.com/search?expandRefinements=true&q='+query+'&concept=a39a4558-f562-4dca-8774-000246e6eebe&dateFrom='+Date.strftime(\"20%y-%m-%d\")+'&dateTo='+Date.strftime(\"20%y-%m-%d\")+'&page='+str(page)\n",
    "\n",
    "            #Request the URL and Retrieve the Headlines\n",
    "            raw_html = requests.get(URL, headers=Headers)\n",
    "            soup = BeautifulSoup(raw_html.text, 'html.parser')\n",
    "            headline_list = soup.find_all(\"div\", {\"class\": \"o-teaser__heading\"})\n",
    "\n",
    "            #Define the Date & Headline Lists\n",
    "            dates = []\n",
    "            headlines = []\n",
    "\n",
    "            #Loop through each element in the headline list (for that specific query)\n",
    "            for elem in headline_list:\n",
    "                titles = elem.findAll(text=True)\n",
    "                titles_string = ''.join(titles)\n",
    "                headlines.append(titles_string)   \n",
    "                \n",
    "            #Loop through each element in the headline list (for that specific query)\n",
    "            for elem in headline_list:\n",
    "                dates.append(Date)\n",
    "\n",
    "            #Create a new Dataframe that stores the data of this specific query\n",
    "            News = pd.DataFrame({'Date': dates, 'Headline': headlines})\n",
    "            \n",
    "            #Concatanate this dataframe with the one containing ALL Financial Times News\n",
    "            PD_Headers = pd.concat([PD_Headers, News])\n",
    "    \n",
    "#Remove any possible duplicates\n",
    "PD_Headers.drop_duplicates(['Headline'], keep='last')\n",
    "\n",
    "#Filter the Keywords\n",
    "PD_Headers = PD_Headers[PD_Headers['Headline'].str.contains('|'.join(filter_keywords), case=False)]\n",
    "\n",
    "#Extract the dataframe to a CSV file\n",
    "PD_Headers.to_csv('files/'+query_list[0]+'_News_FT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining ALL News Files into ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load all News sources\n",
    "BS_data = pd.read_csv('files/'+query_list[0]+'_News_BS.csv')\n",
    "FT_data = pd.read_csv('files/'+query_list[0]+'_News_FT.csv')\n",
    "NYTimes_data = pd.read_csv('files/'+query_list[0]+'_News_NYTimes.csv')\n",
    "\n",
    "#Set the Data Source to BS\n",
    "BS_data = BS_data.assign(Source='BS')\n",
    "BS_data = BS_data[['Date', 'Headline', 'Source']]\n",
    "\n",
    "#Set the Data Source to FT\n",
    "FT_data = FT_data.assign(Source='FT')\n",
    "FT_data = FT_data[['Date', 'Headline', 'Source']]\n",
    "\n",
    "#Set the Data Source to NYTimes\n",
    "NYTimes_data = NYTimes_data.assign(Source='NYTimes')\n",
    "NYTimes_data = NYTimes_data[['Date', 'Headline', 'Source']]\n",
    "\n",
    "#Concatenate all data together\n",
    "all_data = pd.concat([FT_data, NYTimes_data, BS_data])\n",
    "\n",
    "all_data.loc[:, 'Date'] = pd.to_datetime(all_data['Date'], format='%m/%d/%Y')\n",
    "\n",
    "#Remove any possible duplicates\n",
    "all_data.drop_duplicates(['Headline'], keep='last')\n",
    "\n",
    "#Extract the dataframe to a CSV file\n",
    "all_data.to_csv('files/'+query_list[0]+'_News_All.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram showing Headlines per Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEoCAYAAACZ5MzqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWVklEQVR4nO3dfbBlVX3m8e8jrQIqAtIapiE2xi4VX4EWiOYVDIIxQhJNMEZ7CElnMiTqmKoR1BqMhiqtqEQmI0qE2Bgj4ksCKU2gRTFOTRQbJLyIhh5loIVIa6OoKNjwmz/Ounptb3efu7j37D7c76fq1t177bX3/p26Xf2cvfZbqgpJkno8aOgCJEnTyxCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1WzZ0AZO233771cqVK4cuQ5KmxpVXXvn1qlo+17IlFyIrV65kw4YNQ5chSVMjyf/b3jKHsyRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0WLUSSnJfk9iTXzWrbN8n6JDe23/u09iQ5K8nGJNckOXTWOmta/xuTrJnVfliSa9s6ZyXJYn0WSdLcFvNI5D3Asdu0nQpcVlWrgMvaPMBxwKr2sxY4G0ahA5wOHAEcDpw+Ezytz9pZ6227L0nSIlu0EKmqfwG2bNN8PLCuTa8DTpjVfn6NfAbYO8n+wHOB9VW1paruANYDx7Zle1XVv9bohSjnz9qWJGlCJn2z4WOq6jaAqrotyaNb+wrglln9NrW2HbVvmqN9l7Ly1I8OXcKiuulNvzp0CZIGtqucWJ/rfEZ1tM+98WRtkg1JNmzevLmzREnStiYdIl9rQ1G037e39k3AgbP6HQDcupP2A+Zon1NVnVNVq6tq9fLlcz7+RZLUYdIhcjEwc4XVGuCiWe0va1dpHQl8qw17XQIck2SfdkL9GOCStuzbSY5sV2W9bNa2JEkTsmjnRJK8H/glYL8kmxhdZfUm4MIkJwM3Ay9q3T8GPA/YCNwFnARQVVuSvBH4XOv3hqqaOVn/R4yuANsD+Kf2I0maoEULkap68XYWHT1H3wJO2c52zgPOm6N9A/CU+1OjJOn+2VVOrEuSppAhIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuk36pVTS1PClYtLOGSKSHpAeyF8CdqUvAA5nSZK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqdsgIZLkvyW5Psl1Sd6fZPckByX5bJIbk3wgyUNa34e2+Y1t+cpZ2zmttX8pyXOH+CyStJRNPESSrABeDqyuqqcAuwEnAm8GzqyqVcAdwMltlZOBO6rq8cCZrR9JDm7rPRk4FnhHkt0m+VkkaakbajhrGbBHkmXAnsBtwFHAh9rydcAJbfr4Nk9bfnSStPYLquruqvoKsBE4fEL1S5IYIESq6qvAW4CbGYXHt4ArgW9W1dbWbROwok2vAG5p625t/R81u32OdSRJEzDEcNY+jI4iDgL+E/Aw4Lg5utbMKttZtr32ufa5NsmGJBs2b948/6IlSXMaYjjrOcBXqmpzVf0A+AjwLGDvNrwFcABwa5veBBwI0JY/Etgyu32OdX5MVZ1TVauravXy5csX+vNI0pI1RIjcDByZZM92buNo4AvAJ4EXtj5rgIva9MVtnrb8E1VVrf3EdvXWQcAq4IoJfQZJEqMT3BNVVZ9N8iHgKmAr8HngHOCjwAVJ/ry1ndtWORd4b5KNjI5ATmzbuT7JhYwCaCtwSlXdO9EPI0lL3MRDBKCqTgdO36b5y8xxdVVVfR940Xa2cwZwxoIXKEkai3esS5K6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSp21ghkuQpi12IJGn6jHsk8s4kVyT5r0n2XtSKJElTY6wQqaqfA14CHAhsSPJ3SX5lUSuTJO3yxj4nUlU3Aq8DXg38InBWki8m+Y3FKk6StGsb95zI05KcCdwAHAX8WlU9qU2fuYj1SZJ2YcvG7PdXwF8Dr6mq7800VtWtSV63KJVJknZ54w5nPQ/4u5kASfKgJHsCVNV757vTJHsn+VAbDrshyc8m2TfJ+iQ3tt/7tL5JclaSjUmuSXLorO2saf1vTLJmvnVIku6fcUPk48Aes+b3bG293g78c1U9EXg6o2GyU4HLqmoVcFmbBzgOWNV+1gJnAyTZFzgdOAI4HDh9JngkSZMxbojsXlXfmZlp03v27DDJXsAvAOe2bd1TVd8EjgfWtW7rgBPa9PHA+TXyGWDvJPsDzwXWV9WWqroDWA8c21OTJKnPuCHy3W2GkQ4DvreD/jvyOGAz8DdJPp/k3UkeBjymqm4DaL8f3fqvAG6Ztf6m1ra99p+QZG2SDUk2bN68ubNsSdK2xg2RVwIfTPLpJJ8GPgD8cec+lwGHAmdX1SHAd/nR0NVcMkdb7aD9Jxurzqmq1VW1evny5fOtV5K0HWNdnVVVn0vyROAJjP7z/mJV/aBzn5uATVX12Tb/IUYh8rUk+1fVbW246vZZ/Q+ctf4BwK2t/Ze2ab+8syZJUof5PIDxmcDTgEOAFyd5Wc8Oq+o/gFuSPKE1HQ18AbgYmLnCag1wUZu+GHhZu0rrSOBbbbjrEuCYJPu0E+rHtDZJ0oSMdSSS5L3AzwBXA/e25gLO79zvnwDvS/IQ4MvASYwC7cIkJwM3Ay9qfT/G6BLjjcBdrS9VtSXJG4HPtX5vqKotnfVIkjqMe7PhauDgqprznMN8VdXVbZvbOnqOvgWcsp3tnAectxA1SZLmb9zhrOuAn1rMQiRJ02fcI5H9gC8kuQK4e6axql6wKFVJkqbCuCHy+sUsQpI0nca9xPdTSR4LrKqqj7fnZu22uKVJknZ14z4K/g8Y3c/xrta0AviHxSpKkjQdxj2xfgrwbOBO+OELqh69wzUkSQ9444bI3VV1z8xMkmVs5xEjkqSlY9wQ+VSS1wB7tHerfxD4x8UrS5I0DcYNkVMZPXn3WuAPGd1F7hsNJWmJG/fqrPsYvR73rxe3HEnSNBn32VlfYY5zIFX1uAWvSJI0Nebz7KwZuzN6OOK+C1+OJGmajHVOpKq+Mevnq1X1l8BRi1ybJGkXN+5w1qGzZh/E6MjkEYtSkSRpaow7nPXWWdNbgZuA31rwaiRJU2Xcq7N+ebELkSRNn3GHs161o+VV9baFKUeSNE3mc3XWMxm97xzg14B/AW5ZjKIkSdNhPi+lOrSqvg2Q5PXAB6vq9xerMEnSrm/cx578NHDPrPl7gJULXo0kaaqMeyTyXuCKJH/P6M71XwfOX7SqJElTYdyrs85I8k/Az7emk6rq84tXliRpGow7nAWwJ3BnVb0d2JTkoEWqSZI0JcZ9Pe7pwKuB01rTg4G/XayiJEnTYdwjkV8HXgB8F6CqbsXHnkjSkjduiNxTVUV7HHyShy1eSZKkaTFuiFyY5F3A3kn+APg4vqBKkpa8ca/Oekt7t/qdwBOA/1FV6xe1MknSLm+nIZJkN+CSqnoOYHBIkn5op8NZVXUvcFeSR06gHknSFBn3jvXvA9cmWU+7Qgugql6+KFVJkqbCuCHy0fYjSdIP7TBEkvx0Vd1cVesWesftXMsG4KtV9fx2B/wFwL7AVcBLq+qeJA9l9Jyuw4BvAL9dVTe1bZwGnAzcC7y8qi5Z6DolSdu3s3Mi/zAzkeTDC7zvVwA3zJp/M3BmVa0C7mAUDrTfd1TV44EzWz+SHAycCDwZOBZ4RwsmSdKE7CxEMmv6cQu10yQHAL8KvLvNBzgK+FDrsg44oU0f3+Zpy49u/Y8HLqiqu6vqK8BG4PCFqlGStHM7C5HazvT99ZfAfwfua/OPAr5ZVVvb/CZgRZteQXuDYlv+rdb/h+1zrCNJmoCdhcjTk9yZ5NvA09r0nUm+neTOnh0meT5we1VdObt5jq61k2U7Wmfbfa5NsiHJhs2bN8+rXknS9u3wxHpVLcY5hmcDL0jyPGB3YC9GRyZ7J1nWjjYOAG5t/TcBBzJ6/Pwy4JHAllntM2avs+3nOAc4B2D16tULeUQlSUvafN4nsiCq6rSqOqCqVjI6Mf6JqnoJ8Engha3bGuCiNn1xm6ct/0R7GOTFwIlJHtqu7FoFXDGhjyFJYvz7RCbh1cAFSf4c+Dxwbms/F3hvko2MjkBOBKiq65NcCHwB2Aqc0u6ulyRNyKAhUlWXA5e36S8zx9VVVfV94EXbWf8M4IzFq1CStCMTH86SJD1wGCKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqNvEQSXJgkk8muSHJ9Ule0dr3TbI+yY3t9z6tPUnOSrIxyTVJDp21rTWt/41J1kz6s0jSUjfEkchW4E+r6knAkcApSQ4GTgUuq6pVwGVtHuA4YFX7WQucDaPQAU4HjgAOB06fCR5J0mRMPESq6raquqpNfxu4AVgBHA+sa93WASe06eOB82vkM8DeSfYHngusr6otVXUHsB44doIfRZKWvEHPiSRZCRwCfBZ4TFXdBqOgAR7duq0Abpm12qbWtr12SdKEDBYiSR4OfBh4ZVXduaOuc7TVDtrn2tfaJBuSbNi8efP8i5UkzWmQEEnyYEYB8r6q+khr/lobpqL9vr21bwIOnLX6AcCtO2j/CVV1TlWtrqrVy5cvX7gPIklL3BBXZwU4F7ihqt42a9HFwMwVVmuAi2a1v6xdpXUk8K023HUJcEySfdoJ9WNamyRpQpYNsM9nAy8Frk1ydWt7DfAm4MIkJwM3Ay9qyz4GPA/YCNwFnARQVVuSvBH4XOv3hqraMpmPIEmCAUKkqv43c5/PADh6jv4FnLKdbZ0HnLdw1UmS5sM71iVJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndpj5Ekhyb5EtJNiY5deh6JGkpmeoQSbIb8L+A44CDgRcnOXjYqiRp6ZjqEAEOBzZW1Zer6h7gAuD4gWuSpCVj2dAF3E8rgFtmzW8Cjti2U5K1wNo2+50kX5pAbUPYD/j6pHaWN09qT0uGf7/pNrG/3wB/u8dub8G0h0jmaKufaKg6Bzhn8csZVpINVbV66DrUx7/fdFuqf79pH87aBBw4a/4A4NaBapGkJWfaQ+RzwKokByV5CHAicPHANUnSkjHVw1lVtTXJHwOXALsB51XV9QOXNaQH/JDdA5x/v+m2JP9+qfqJUwiSJI1l2oezJEkDMkQkSd0MEUlSN0NEkuYpyXuGrmFXYYhMqSSPTfLIWfO/nOTtSV7VLnfWFEjyiiR7ZeTcJFclOWbourRTTxu6gF2FITK9LgQeBpDkGcAHgZuBpwPvGLAuzc/vVdWdwDHAcuAk4E3DlqQx7JnkkCSHzvUzdHGTNNX3iSxxe1TVzN35v8voHpm3JnkQcPWAdWl+Zh7d8zzgb6rq35LM9Tgf7VpWAG9l+49eOmqy5QzHEJles//xHgWcBlBV9/l/0FS5MsmlwEHAaUkeAdw3cE3auY1VtWSCYkcMken1iSQXArcB+wCfAEiyP3DPkIVpXk4GngF8uaruSvIoRkNa0lTwnMj0eiXwEeAm4Oeq6get/aeA1w5VlOatGL1Q7eVt/mHA7sOVozGdPXQBuwofe/IAkmQ/4BvlH3VqJDmb0fDVUVX1pCT7AJdW1TMHLk07kOSqqjq0TX+4qn5z6JqG4pHIlEpyZJLLk3ykXSVyHXAd8LUkxw5dn8Z2RFWdAnwfoKruALxEe9c3+8Tj4warYhfgOZHp9VfAa4BHMjofclxVfSbJE4H3A/88ZHEa2w+S7EZ7mVqS5XhifRrUdqaXHENkei2rqksBkryhqj4DUFVf9OqsqXIW8PfAo5OcAbwQeN2wJWkMT09yJ6Mjkj3aNG2+qmqv4UqbLENkes3+tvq9bZYt6W9G06Sq3pfkSuBoRv8BnVBVNwxclnaiqnYbuoZdhSfWp1SSe4Hv0r4JAXfNLAJ2r6oHD1Wb5qedTD+QWV/qquqq4SqSxmeISANK8kbgPwP/lx8dQZY3smlaGCLSgJJ8CXhqVXmDqKaSl/hKw7oO2HvoIqReHolIA0qyGriIUZjcPdNeVS8YrChpHrw6SxrWOuDNwLV4f4imkCEiDevrVXXW0EVIvRzOkgaU5G2MhrEu5seHs7zEV1PBEJEGlOSTczR7ia+mhiEiSermORFpAEl+t6r+Nsmr5lpeVW+bdE1SD0NEGsae7fcjBq1Cup8MEWkYDwGoqj8buhDp/vCOdWkYvzd0AdJCMEQkSd28OksaQJKt/Ojx/T+2iCX2UiNNN8+JSMO4tqoOGboI6f5yOEuS1M0QkYZx4dAFSAvBEJGG8VtJ3pXEd4loqhki0jAOA24Arkjy0qGLkXp5dZY0oCQHA//K6Atd4dVZmjIeiUgDSXIyo7cavhbYq6r2qqpHGCCaJl7iKw0gyf8BbgJ+vqr+Y+BypG4OZ0kDSPIrVbV+6Dqk+8sjEWkYz0rys9tZVlX1xolWI3XySEQaQJI/naN5T+D3gUdV1cMnXJLUxRCRBpbkEcArgJMZ3YT41qq6fdiqpPE4nCUNJMm+wKuAlwDrgEOr6o5hq5LmxxCRBpDkL4DfAM4BnlpV3xm4JKmLw1nSAJLcB9wNbGV0k+EPF+HNhpoihogkqZt3rEuSuhkikqRuhoi0QJK8Nsn1Sa5JcnWSI4auSVpsXp0lLYB29/nzGV2me3eS/YCH3M9tLquqrQtSoLRIPBKRFsb+wNer6m6Aqvp6Vd2a5Ogkn09ybZLzkjwUIMlNLWhIsjrJ5W369UnOSXIpcH6S3ZK8pa1/TZI/af0OS/KpJFcmuSTJ/oN8ai15hoi0MC4FDkzy70nekeQXk+wOvAf47ap6KqMj/z8aY1uHAcdX1e8Aa4GDgEOq6mnA+5I8GPifwAur6jDgPOCMhf9I0s4ZItICaDcLHsboP/3NwAeAPwS+UlX/3rqtA35hjM1dXFXfa9PPAd45M6xVVVuAJwBPAdYnuRp4HXDAQn0WaT48JyItkKq6F7gcuDzJtcCaHXTfyo++xO2+zbLvzpoOP34z4kzb9VW1vacASxPjkYi0AJI8IcmqWU3PAL4GrEzy+Nb2UuBTbfomRkcuAL+5g01fCvyXJMvafvYFvgQsn3mUfJIHJ3nygnwQaZ4MEWlhPBxYl+QLSa4BDgZOBU4CPtiOTO4D3tn6/xnw9iSfBu7dwXbfDdwMXJPk34Dfqap7gBcCb25tVwPPWowPJe2Mjz2RJHXzSESS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUrf/DzF3C8FDyP/+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "AX = all_data['Source'].value_counts().plot(kind='bar')\n",
    "AX.set_xlabel('Source')\n",
    "AX.set_ylabel('Frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
